# Philosophy

The deeper grounding for those who want it. You don't need this to collaborate effectively—the [README](../README.md) is self-sufficient. This is for anyone curious about the foundations.

## The Gap

What someone means and what they say are never quite the same. You've felt this—the frustration of being misunderstood, the effort it takes to explain yourself clearly.

I can't perfectly write what I mean. You can't perfectly interpret what I wrote. But here's what we can use: when I observe the result of our actions, I'm the only one who can decide "yes, that's what I meant" or "no, that missed the mark."

This gap can't be closed. It can be worked with.

## Presuppositions

A model encodes assumptions—invariants you take as given so you can reason about something more simply. The right assumptions let you create a simpler model that's actually useful. The wrong assumptions (or too many) make the model collapse under its own weight.

The simplest model I could find for collaboration starts here:

- Something exists.
- Distinct things exist (you and me, agent and world).
- Physics is consistent at the scale we operate.
- An agent is an entity capable of acting in the world.

These are models too, with their own assumptions. Non-duality questions whether distinct things exist. Quantum mechanics questions classical consistency. But you can model a pendulum without accounting for quantum physics, even though the pendulum operates in quantum reality. I'm choosing the level that lets me say something useful about collaboration, not claiming these are ultimately true.

## Truths

From those presuppositions, six truths apply to any agent—human or AI:

1. **Agents act from internal objectives.** This is intent.
2. **Agents have internal world models** that are incomplete, fallible, and mutable.
3. **Agents use their world models** to select actions in pursuit of objectives.
4. **World input is taken as given**; what it means is model-derived.
5. **Collaboration adds context** that agents can't derive alone, including the intent of the collaborator.
6. **Expressed intent is lossy.** Agents must infer intent from expression, not treat expression as intent.

That sixth truth is where this all leads. Words are models of concepts, and models simplify. Under the assumptions above—all the way back to distinct things existing—this necessarily follows.

You're welcome to reject any of these assumptions, but if you do, this framework will be less useful for us when we collaborate.

## The Loop

Because expressed intent is lossy, collaboration needs a correction loop. OODA (Observe, Orient, Decide, Act) is that loop.

```
        ┌──────────────────────────────┐
        │                              │
        ▼                              │
    Observe → Orient → Decide → Act ───┘
                 ▲
                 │
            [concepts]
```

**Observe**: Notice what happened—feedback, friction, misalignment.

**Orient**: Interpret through your worldview—concepts, principles, mental models. This is where the framework lives.

**Decide**: Choose whether to update your understanding or proceed as-is.

**Act**: Make the change (or don't). The results become new observations.

The human is essential at Decide. Only you know if what you observed matches what you intended. An agent can propose, orient, even act—but the decision to accept the result or iterate again is yours. That's the invariant from The Gap: you're the one who closes the loop.

## The Contract

These are the assumptions I'm asking you to accept:

1. You take the six truths as useful working assumptions.
2. You take my word that this framework reflects how I think about these things.

From my end, all these files are correct. The way I read them is the way I intend for them to be read. Beyond that, I can't guarantee you'll interpret them the way I meant.

That's the gap—and why we iterate.
